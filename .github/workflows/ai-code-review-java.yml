name: Codex AI Code Review (Java)

on:
  workflow_call: {}

permissions:
  contents: read
  pull-requests: write

concurrency:
  group: ai-review-java-${{ github.repository }}-${{ github.event.pull_request.number || github.run_id }}
  cancel-in-progress: true

jobs:
  ai_review:
    name: AI Code Review
    if: ${{ github.event_name != 'pull_request' || !github.event.pull_request.draft }}
    runs-on: ubuntu-latest

    env:
      AI_PROVIDER: openai
      AI_TEMPERATURE: "0.3"
      MAX_COMMENTS: "12"     # a bit lower to reduce output tokens
      APPROVE_REVIEWS: "false"

      # Tighter excludes to keep payload lean
      EXCLUDE_PATTERNS: >-
        **/*.md,**/*.json,**/target/**,**/build/**,**/*.class,**/*.log,
        **/.mvn/wrapper/**,**/gradle/wrapper/**,**/generated-sources/**,**/build/generated/**,
        **/*.jar,**/*.war,**/*.png,**/*.jpg,**/*.jpeg,**/*.gif,**/*.svg,**/*.mp4,
        **/src/test/**,**/*Test.java,**/*IT.java

      PROJECT_CONTEXT: |
        Java Spring Boot service (Maven).
        - Constructor injection; validate inputs.
        - Handle exceptions carefully; avoid logging secrets/PII.
        - Thread-safety & performance in hot paths.
        - Fast, isolated tests.

      # Preflight “token-ish” caps (~4 chars ≈ 1 token; we x2 for old+new)
      MAX_TOTAL_BYTES: "120000"     # ~30k tokens input budget
      MAX_FILE_BYTES:  "30000"      # ignore files larger than this for estimate
      SOFT_SWITCH_TOKENS: "20000"   # switch to mini model above this
      HARD_SKIP_TOKENS:  "60000"    # skip AI step above this

    steps:
      - name: Checkout PR code
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Detect relevant changes
        id: changes
        run: |
          git fetch origin ${{ github.base_ref }} --quiet || true
          if git diff --quiet origin/${{ github.base_ref }} HEAD -- src/main/java/ src/main/resources/; then
            echo "no_changes=true" >> $GITHUB_OUTPUT
          else
            echo "no_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Collect changed files (filtered)
        if: steps.changes.outputs.no_changes == 'false'
        id: files
        run: |
          git diff --name-only origin/${{ github.base_ref }} HEAD -- src/main/java/ src/main/resources/ \
          | grep -v -E '\.lock$|\.json$|\.md$|\.ya?ml$|\.svg$|\.png$|\.jpe?g$|\.gif$|\.mp4$|^target/|^build/|/src/test/|Test\.java$|IT\.java$' \
          > files.txt
          echo "files<<EOF" >> $GITHUB_OUTPUT
          cat files.txt >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Preflight estimate token load and choose model
        if: steps.changes.outputs.no_changes == 'false'
        id: preflight
        env:
          MAX_TOTAL_BYTES:      ${{ env.MAX_TOTAL_BYTES }}
          MAX_FILE_BYTES:       ${{ env.MAX_FILE_BYTES }}
          SOFT_SWITCH_TOKENS:   ${{ env.SOFT_SWITCH_TOKENS }}
          HARD_SKIP_TOKENS:     ${{ env.HARD_SKIP_TOKENS }}
        run: |
          set -euo pipefail

          bytes=0
          counted=0
          smallest_sz=999999999
          while IFS= read -r f; do
            [ -z "$f" ] && continue
            [ -f "$f" ] || continue
            sz=$(wc -c <"$f" | tr -d ' ')
            # ignore huge singles in estimate; the action often chunks anyway
            if [ "$sz" -gt "$MAX_FILE_BYTES" ]; then
              echo "⏭️  Ignoring for estimate: $f ($sz > $MAX_FILE_BYTES)"
              continue
            fi
            bytes=$((bytes + sz))
            counted=$((counted + 1))
            [ "$sz" -lt "$smallest_sz" ] && smallest_sz="$sz"
          done < <(printf "%s\n" "${{ steps.files.outputs.files }}")

          # very rough token estimate: (bytes / 4) * 2 (old+new) + overhead
          input_tokens=$(( (bytes / 4) * 2 + 2000 ))

          # choose model
          default_model="${{ vars.AI_MODEL || 'gpt-4.1' }}"
          model="$default_model"
          if [ "$input_tokens" -gt "$SOFT_SWITCH_TOKENS" ]; then
            model="gpt-4.1-mini"
          fi

          # decide run/skip
          run_ai="true"
          if [ "$input_tokens" -gt "$HARD_SKIP_TOKENS" ]; then
            run_ai="false"
          fi

          echo "bytes=$bytes"                >> $GITHUB_OUTPUT
          echo "files_count=$counted"         >> $GITHUB_OUTPUT
          echo "estimated_tokens=$input_tokens" >> $GITHUB_OUTPUT
          echo "selected_model=$model"        >> $GITHUB_OUTPUT
          echo "run_ai=$run_ai"               >> $GITHUB_OUTPUT

          {
            echo "### AI Review Preflight"
            echo "- Files counted: **$counted**"
            echo "- Estimated input tokens: **$input_tokens**"
            echo "- Selected model: **$model** (default: $default_model)"
            echo "- Decision: **$run_ai** (skip if false)"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Jitter to reduce org-wide bursts (5–25s)
        if: steps.preflight.outputs.run_ai == 'true'
        run: |
          sleep $(( (RANDOM % 21) + 5 ))

      - name: Run AI Code Review
        if: steps.preflight.outputs.run_ai == 'true'
        id: ai_review
        uses: Eshopbox-Enginnering/ai-codereviewer@main
        with:
          GITHUB_TOKEN:   ${{ secrets.GITHUB_TOKEN }}
          AI_PROVIDER:    ${{ env.AI_PROVIDER }}
          AI_API_KEY:     ${{ secrets.AI_REVIEW_CODEX }}
          AI_MODEL:       ${{ steps.preflight.outputs.selected_model }}
          AI_TEMPERATURE: ${{ env.AI_TEMPERATURE }}
          APPROVE_REVIEWS:${{ env.APPROVE_REVIEWS }}
          MAX_COMMENTS:   ${{ env.MAX_COMMENTS }}
          PROJECT_CONTEXT:${{ env.PROJECT_CONTEXT }}
          EXCLUDE_PATTERNS:${{ env.EXCLUDE_PATTERNS }}

      - name: Skip AI (payload too large)
        if: steps.preflight.outputs.run_ai != 'true'
        run: |
          echo "::warning title=AI review skipped::Estimated ${{
            steps.preflight.outputs.estimated_tokens
          }} tokens exceeds cap ${{
            env.HARD_SKIP_TOKENS
          }}. Consider slicing the PR or running a manual scan."
